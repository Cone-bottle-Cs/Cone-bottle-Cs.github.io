<!DOCTYPE html>
<html>
<head>
<title>语音识别笔记.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="span-style%22displayblocktext-aligncentercolorlightblue%22%E6%90%93%E4%B8%80%E4%B8%AA%E8%AF%AD%E9%9F%B3%E5%94%A4%E9%86%92span"><span style="display:block;text-align:center;color:lightblue;">搓一个语音唤醒</span></h1>
<p>（哈哈 这个排版 稀烂咯）</p>
<h2 id="%E7%9B%AE%E6%A0%87">目标</h2>
<p>搓一个语音唤醒（keyword spotting），用于上课时摸鱼，老师点名时可以做出提醒，并且保留点名前后的一部分音频（毕竟还得知道为啥点你）。只需要识别名字，因此输入长度为1秒左右，约为两到四个音节，输出为两个类别的概率。计划采用简单的CNN+线性，将图输入进来卷积+池化然后激活，过两遍，采用适应性池化压缩为$图数\times1\times1$的向量，进两层线性+激活，最后输出为二维向量。</p>
<h2 id="%E9%A6%96%E5%85%88%E5%81%9A%E5%87%BA%E5%9B%BE%E5%83%8F%E6%9D%A5">首先做出图像来</h2>
<p>取一个音轨，使用快速傅里叶变换来将音频变成频谱图。首先，用<code>torchaudio.load来load一个.wav</code>，返回第一个参数是波形，第二个是采样率，用<code>torchaudio.transforms.MelSpectrogram(这里填上面解析出来的波形)</code>（梅尔图）。其实最好输入前做个滤波之类的预处理，我是因为训练的时候发现有大量的极大值（其他都是零点几，有几个10几的）所以用了些奇怪的函数来（就是log的一个变形），然后进行一个归一化（用的是最大最小值归一化）</p>
<h2 id="%E7%84%B6%E5%90%8E%E4%B8%A2%E7%BB%99%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%88%E7%BB%84%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C">然后丢给神经网络，先组一个网络</h2>
<p>由于没有经验，加上torch本身的教程和中文教程都大同小异地看不懂，所以一边摸索一边做（但是笔记是写完才做的。。。所以基本上忘记踩了什么坑了 悲）<br>
aF采用elu函数（变形的泄露relu），卷积层1为<u>1层入4层出7核格宽</u>，经过(2,2)的最大池化，卷积层2为<u>4层入6层出5核格宽</u>，经过AdaptiveMaxPool2d变为$6\times1\times1$向量，线性层1<u>输入6输出8</u>，线性层2<u>输入8输出2</u>，net本身结构为</p>
<pre class="hljs"><code><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(x)</span>:</span>
    x=maxPool(aF(cov1(x)))
    x=adaptivePool(aF(cov2(x)))
    x=x.view(<span class="hljs-number">1</span>,<span class="hljs-number">-1</span>)<span class="hljs-comment">#这个是压缩为二维向量</span>
    x=aF(linear1(x))
    x=sigmoid(linear2(x))<span class="hljs-comment">#主要是考虑到这是个概率，所以用个sigmoid变换到0~1</span>
    <span class="hljs-keyword">return</span> x
</div></code></pre>
<h2 id="%E5%89%8D%E5%90%91%E5%92%8C%E5%8F%8D%E5%90%91">前向和反向</h2>
<p>首先先将输入的东西变成tensor，丢进去forward输出用个概率，和标准比对，开始是反向更新。首先先重置优化器的梯度，optimizer用的是optim.SGD(net.parameters(),lr)，其中net.parameters()是需要更新的参数，lr是learningRate（具体参数自己找pytorch，笑死，毫无指引）。loss采用交叉熵<code>nn.CrossEntropyLoss</code>，参数为两个float，我是把两个tensor给.float再导进去。然后对loss进行一个.backward()来取梯度，再optimizer.step()来进行一个步进更新。我最后面返回了一个loss.item()，问题是这个是啥我还没看懂，说是loss但是很不平滑。</p>
<h2 id="%E5%B0%86%E9%9F%B3%E9%A2%91%E6%B5%81%E5%88%87%E7%89%87%E4%B8%A2%E8%BF%9B%E5%8E%BB%E5%88%A4%E6%96%AD%E6%9C%89%E6%B2%A1%E6%9C%89%E6%BF%80%E6%B4%BB">将音频流切片丢进去判断有没有激活</h2>
<p>就是验证部分，取了数据集里面的另一部分来进行检验，我做出来是0.93的准确率。</p>
<h2 id="%E4%BB%8E%E5%A4%96%E9%83%A8%E8%BE%93%E5%85%A5%E9%9F%B3%E9%A2%91%E6%B5%81">从外部输入音频流</h2>
<p>使用pyaudio库。首先是用<code>audio=pyaudio.PyAudio()</code>初始化一个类变量，调用函数<code>stream=audio.open(参数)</code>进行初始化设置，调用<code>data=stream.read(参数)</code>来获取音频流，音频流是记为两位十六进制数的二进制字符串，我是用<code>np.frombuffer(stream.read(这里是frames_per_buffer),dtype=np.float32)</code>来变换的，记得因为是实时音频流所以写while里面，至于这堆二进制怎么变成波形图就靠np罢（称赞一下写综述的大大，该方法经过一系列采坑后通过<a href="zhuanlan.zhihu.com/p/611234973">综述</a>找到的。<br>
采集设备也比较重要，好的麦克风可以先处理掉一部分噪声，收音也更集中。一开始用耳麦做的时候噪点非常多，回来换了舍友的麦之后梅尔图干净了不少。但是还是有个问题，我是按处理.wav输入的方式处理<code>np.frombuffer</code>解析的数据，既把它的返回直接当成时域的信号，清洗后输入转换成梅尔图，再与先前获取的底噪的梅尔图相减。结果上看，结果归一化后，较大值比较多，较小值比较少，与从.wav中获取的梅尔图恰相反。</p>
<h2 id="%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E5%8F%AA%E8%83%BD%E6%83%B3%E8%B5%B7%E4%B8%80%E4%BA%9Bli">遇到的问题（只能想起一些li）</h2>
<p>比如说最开始的梅尔图，会导致输入的图大部分数在0~1少部分在10以上或者-10以下，归一化之后就会导致大部分数挤在中间（当然，你不归一化就没有这个烦恼了 大概 我懒得优化了 <s>反正也是堆屎山</s>），所有没用原本的log2函数，而且有负数，所以考虑了用有负数部分的泄露relu函数变形。<br>
然后还有就是如果发现自己训练的RC比例和样本比例相同那可能就是你样本有问题（别问我怎么知道的），我训练目标用的是<a href="http://www.openslr.org/85/">北京希尔贝壳的HIMIA数据集</a>（吐槽一下，我发现为什么英文只有女声，是我查找的问题吗？），训练的非目标样本是随便截的音频。<br>
我使用<code>nn.AdaptiveMaxPool2d</code>是考虑了输入的音频长度可能有一定范围的差别，所以加的，你当然可以不加，你喜欢的话awa
一开始做那个外部音频流的时候就只找到pyaudio能采集，然后一开始用np.fromstring来处理二进制发现经过很奇怪，没法转换到tensor中进行MelSpectrogram，所以又试了wave库，本来要试ffmpeg的，结果发现Python上的ffmpeg库是只能处理视频的半成品，然后就放弃了去查综述。
MelSpectrogram的语法是<code>MelSpectrogram(参数)(tensor形式的输入)</code>，如果报了int is not callable之类的（具体忘了x）那应该是MelSpectrogram的输入你搞错了，请检查一下输入是否为tensor，以及维度（？这个不确定会不会是）</p>
<style>
        /*黑幕组件*/
        
        .black {
            display: inline;
            background-color: rgba(0, 0, 0, 1);
        }
        
        .black:hover {
            display: inline;
            background-color: rgba(0, 0, 0, 0);
        }
</style>
<script>
    /*LaTeX组件*/
        MathJax = {
            // 仅仅为标志符，表示在这个符号中间的内容为公式内容，官方文档多了['(',')']
            tex: {
                inlineMath: [
                    ['$', '$']
                ]
            }
        };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
</body>
</html>
