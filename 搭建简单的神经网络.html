<!DOCTYPE html>
<html>
<head>
<title>搭建简单的神经网络.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="span-style%22displayblocktext-aligncentercolorlightblue%22%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E7%BD%91%E7%BB%9Cspan"><span style="display:block;text-align:center;color:lightblue;">搭建一个简单的网络</span></h1>
<h2 id="%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2">写在前面</h2>
<h3 id="%E7%9E%8E%E6%89%AF">瞎扯</h3>
<p>我们终究不是做研究的，因此对于神经网络，只要会看流程图然后直接搭，或者直接调包就行了，所以我们需要搞明白的是：有什么输入、有什么输出、需要什么模块。本文将讲述Pytorch和TensorFlow的基础组件，用他们<strong>搭建</strong>出一个简单的神经网络，并进行<strong>量化</strong>。本文还会对比两个库一些操作的异同（都是坑啊），并展示几种模型的转换。TensorFlow主要是使用Keras框架和TensorFlow Lite（好吧，其实都是在TensorFlow库）</p>
<h3 id="%E4%BD%A0%E9%9C%80%E8%A6%81%E7%9A%84%E5%9F%BA%E7%A1%80">你需要的基础</h3>
<p>你需要知道Python的基础语法，有Numpy的基础，知道什么是卷积，以及卷积的相关知识。文章将会先讲解两个库的组件，如何搭建神经网络，如何进行<a href="#%E4%BB%80%E4%B9%88%E6%98%AF%E9%87%8F%E5%8C%96">量化</a>。</p>
<h3 id="%E6%95%B0%E6%8D%AE%E7%9A%84%E7%BB%93%E6%9E%84">数据的结构</h3>
<p>在这里，我们以图像分类为例，在我们传统认知中，一张图的参数有长宽和通道数（就是RGB三通道啦），所以图像分类的输入就是一个三维数组，吗？可以，但是一般我们不会只输入一张图去训练，一般来说会一个批（batch）一起训练，所以就还有一个参数<strong>batchSize</strong>，也就是说一般是一个四维数组。<br>
需要注意的是，<strong>Pytorch和TensorFlow的输入结构不同</strong>，Pytorch的输入为<code>(batchSize,channel,height,width)</code>既NCHW，而TensorFlow为<code>(batchSize,height,width,channel)</code>既NHWC。<br>
<a href="https://blog.csdn.net/feng_xun123/article/details/108960632">为什么两个库的存储结构不同呢？</a></p>
<h3 id="%E6%95%B0%E6%8D%AE%E7%9A%84%E7%B1%BB%E5%9E%8B">数据的类型</h3>
<p>就是<strong>dtype</strong>啦，Pytorch用的是<code>torch.</code>比如<code>torch.uint8</code>，这是因为Pytorch的数据都是以<code>torch.Tensor</code>的形式存在的（可以理解为多维数组）。而TensorFlow可能直接用numpy的ndarray，这是因为TensorFlow可以将其自动转化为<code>tf.Tensor</code>，所以其dtype也是numpy的，比如<code>numpy.uint8</code>。</p>
<h3 id="%E8%BF%90%E8%A1%8C%E7%9A%84%E8%AE%BE%E5%A4%87">运行的设备</h3>
<p>就是<strong>device</strong>啦，需要注意的是，Pytorch有CPU和同时兼容GPU/CPU的版本，而在训练时，如果使用GPU进行训练的话，需要对输入进行<code>.to(device)</code>，也需要对模型进行，具体如下：</p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> torch
device=torch.device(<span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)<span class="hljs-comment">#这一句是判断有没有GPU的，语法为torch.device(设备名字)，比如torch.device("cuda1")</span>
model=CNN(inputChannels,width,height,num=<span class="hljs-number">3</span>)
model.load_state_dict(torch.load(<span class="hljs-string">"example"</span>),strict=<span class="hljs-literal">True</span>)<span class="hljs-comment">#这里通过函数.load_state_dict来加载权重等参数</span>
model.to(device)<span class="hljs-comment">#将模型加载到指定的设备上</span>

model.eval()<span class="hljs-comment">#将模型切换为推理状态，避免模型被改变</span>
inputs=torch.randn(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">224</span>,<span class="hljs-number">224</span>)
inputs.to(device)<span class="hljs-comment">#将输入加载到指定的设备上</span>
outputs=model(inputs)
print(<span class="hljs-string">"Output shape:"</span>,outputs)
</div></code></pre>
<p>相应的，如果是TensorFlow可以使用<code>with tf.device(device)</code>：</p>
<pre class="hljs"><code><div><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

device=<span class="hljs-string">"/gpu:0"</span> <span class="hljs-keyword">if</span> tf.test.is_gpu_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"/cpu:0"</span><span class="hljs-comment">#tf.test.is_gpu_available()是判断有没有GPU的</span>

model=CNN(inputChannels,width,height,num=<span class="hljs-number">3</span>)
dummy_input=tf.constant(np.zeros((<span class="hljs-number">1</span>,<span class="hljs-number">224</span>,<span class="hljs-number">224</span>,<span class="hljs-number">3</span>),dtype=np.float32))<span class="hljs-comment">#这里是搞一个虚拟输入，让他初始化模型，输入只需要和你期望的输入形状一样即可，没有值的要求</span>
<span class="hljs-comment">#对了，如果是单通道，比如(1,224,224,1)，你可以只输入(1,224,224)，TensorFlow会自己补齐，这里会在后面的tensorflow.keras.layers.Input函数讲到</span>
model(dummy_input)
model.load_weights(<span class="hljs-string">"weight.h5"</span>)<span class="hljs-comment">#通过函数.load_weights来加载权重</span>

inputs=np.random.rand(<span class="hljs-number">1</span>,<span class="hljs-number">224</span>,<span class="hljs-number">224</span>,<span class="hljs-number">3</span>).astype(np.float32)
<span class="hljs-keyword">with</span> tf.device(device):<span class="hljs-comment">#在设备上进行推理</span>
    inputs=tf.constant(inputs)
    outputs=model(inputs)
print(<span class="hljs-string">"Output shape:"</span>,outputs)
</div></code></pre>
<p>由于需要CPU对GPU进行数据的I/O，如果服务器的CPU拉胯的话会出现瓶颈。<a href="https://www.autodl.com/docs/gpu/">关于这点，可以看看这里（不是广告）。</a></p>
<h2 id="%E6%9C%89%E4%BB%80%E4%B9%88%E7%BB%84%E4%BB%B6">有什么组件</h2>
<p>传统的神经网络可以分为特征提取和分类两个部分，特征提取有卷积、残差、自动编码器等方法，分类部分可以是传统的K临近、支持向量机、全连接。在TensorFlow部分，我们用的是Keras框架，故以下Keras等价于TensorFlow。</p>
<h3 id="%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82">全连接层</h3>
<p>全连接层一般用于分类，放在特征提取部分后面，其输入和输出都是一维的（不考虑batchSize的话）。<br>
<a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"><strong>Pytorch</strong></a></p>
<pre class="hljs"><code><div>torch.nn.Linear(in_features,out_features,bias=<span class="hljs-literal">True</span>,device=<span class="hljs-literal">None</span>,dtype=<span class="hljs-literal">None</span>)
Example:
torch.nn.Linear(<span class="hljs-number">12</span>,<span class="hljs-number">1</span>,bias=<span class="hljs-literal">False</span>)
</div></code></pre>
<p>in_features是指输入的大小<br>
out_features是指输入的大小<br>
bias是bool值，表示是否要开启bias</p>
<blockquote>
<p>什么是bias？就是$\hat{y}=wx+b$的<em>b</em></p>
</blockquote>
<p><a href="#%E8%BF%90%E8%A1%8C%E7%9A%84%E8%AE%BE%E5%A4%87">device</a>和<a href="#%E6%95%B0%E6%8D%AE%E7%9A%84%E7%B1%BB%E5%9E%8B">dtype</a>，上面有说，不再赘述。<br>
<a href="https://keras.io/api/layers/core_layers/dense/"><strong>Keras</strong></a></p>
<pre class="hljs"><code><div>tf.keras.layers.Dense(
    units,
    activation=<span class="hljs-literal">None</span>,
    use_bias=<span class="hljs-literal">True</span>
)<span class="hljs-comment">#我们主要看这三个参数，其余参数见文档</span>
Example:
tf.keras.layers.Dense(<span class="hljs-number">1</span>,activation=<span class="hljs-string">'sigmoid'</span>,use_bias=<span class="hljs-literal">False</span>)
</div></code></pre>
<p>units是指输出的大小，Keras是不管输入的大小的，所以需要对模型进行初始化<br>
activation是指激活函数，和Pytorch不同，Keras没有把激活函数分开（不过Keras也有可单独调用的<a href="https://keras.io/api/layers/activations/">激活函数</a>），而是直接写在参数里，如果不需要激活函数，那就用默认的None即可<br>
use_bias和Pytorch一样，表示是否要开启bias</p>
<h3 id="%E4%BA%8C%E7%BB%B4%E5%8D%B7%E7%A7%AF%E5%B1%82">（二维）卷积层</h3>
<p>我们的目的是做图像分类，所以现在只考虑二维卷积层，下称卷积层。卷积层的作用是提取图像的特征，一个卷积层有多个卷积核，其结果放在不同通道，因此一张单通道的图放进去，出来可能就变成了16通道（有16个不同的卷积核）。<br>
<a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html"><strong>Python</strong></a></p>
<pre class="hljs"><code><div>torch.nn.Conv2d(in_channels,out_channels,kernel_size,stride=<span class="hljs-number">1</span>,padding=<span class="hljs-number">0</span>,dilation=<span class="hljs-number">1</span>,groups=<span class="hljs-number">1</span>,bias=<span class="hljs-literal">True</span>,padding_mode=<span class="hljs-string">'zeros'</span>,device=<span class="hljs-literal">None</span>,dtype=<span class="hljs-literal">None</span>)
Example:
torch.nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,kernel_size=<span class="hljs-number">14</span>)
</div></code></pre>
<p>in_channels,out_channels就是输入输出的通道数<br>
kernel_size是卷积核的大小<br>
stride是卷积核每一步的滑动距离<br>
padding是填充的大小，dilation_mode就是填充方式<br>
dilation和groups是更进一步的内容了，用不上就不赘述了<br>
<a href="https://keras.io/zh/layers/convolutional/"><strong>Keras</strong></a></p>
<pre class="hljs"><code><div>keras.layers.Conv2D(filters,kernel_size,strides=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),padding=<span class="hljs-string">'valid'</span>,data_format=<span class="hljs-literal">None</span>,dilation_rate=(<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),activation=<span class="hljs-literal">None</span>,use_bias=<span class="hljs-literal">True</span>,kernel_initializer=<span class="hljs-string">'glorot_uniform'</span>,bias_initializer=<span class="hljs-string">'zeros'</span>,kernel_regularizer=<span class="hljs-literal">None</span>,bias_regularizer=<span class="hljs-literal">None</span>,activity_regularizer=<span class="hljs-literal">None</span>,kernel_constraint=<span class="hljs-literal">None</span>,bias_constraint=<span class="hljs-literal">None</span>)
Example:
tf.keras.layers.Conv2D(<span class="hljs-number">3</span>,kernel_size=<span class="hljs-number">14</span>)
</div></code></pre>
<p>参数有点多，但是常用的参数总体上和Pytorch基本相似，不同点在于TensorFlow可以让卷积核在两个方向上滑动步长不同，以及和全连接一样可以把激活函数嵌进去。注意filters是卷积核的数量，这是与Pytorch不同的（尽管在样例中他们用相同的值）。</p>
<h3 id="%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E5%B1%82">激活函数层</h3>
<p>这里只考虑常用的两个激活函数：sigmoid和ReLU，他们的形态如下：
<img src="./af.png" alt="激活函数示意图"><br>
sigmoid一般用在需要对输出进行归一化的地方，比如分类中<strong>用sigmoid处理全连接的输出来表示概率</strong>；而如果提取特征的话，一般就用ReLU，比如<strong>卷积层后面接的大多是ReLU</strong>。更多信息请看：<a href="https://blog.csdn.net/hy592070616/article/details/120616475">激活函数汇总1</a>，<a href="https://zhuanlan.zhihu.com/p/70810466">激活函数汇总2</a>。<br>
<a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity"><strong>Pytorch</strong></a></p>
<pre class="hljs"><code><div>self.conv1=torch.nn.Sequential(
    torch.nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,kernel_size=<span class="hljs-number">14</span>),
    torch.nn.ReLU(),
)
</div></code></pre>
<p><a href="https://keras.io/api/layers/activation_layers/"><strong>Keras</strong></a><br>
除了前面提到的可以单独调用的激活函数，还有可以单独调用的激活函数层，我们可以往卷积层后面塞这个，比如</p>
<pre class="hljs"><code><div>self.conv1=tf.keras.Sequential([<span class="hljs-comment">#这个可以把多个层放在一起，方便管理和构造更大的块，之后会讲</span>
    tf.keras.layers.Conv2D(<span class="hljs-number">3</span>,kernel_size=<span class="hljs-number">14</span>),
    tf.keras.layers.ReLU()
])
</div></code></pre>
<h3 id="%E8%BE%93%E5%85%A5%E5%B1%82%E5%BB%BA%E9%80%A0%E4%B8%AD">输入层（建造中）</h3>
<p><a href="https://keras.io/api/layers/core_layers/input/"><strong>Keras</strong></a></p>
<pre class="hljs"><code><div>tf.keras.layers.Input(
    shape=<span class="hljs-literal">None</span>,
    batch_size=<span class="hljs-literal">None</span>,
    name=<span class="hljs-literal">None</span>,
    dtype=<span class="hljs-literal">None</span>,
    sparse=<span class="hljs-literal">None</span>,
    tensor=<span class="hljs-literal">None</span>,
    ragged=<span class="hljs-literal">None</span>.
    type_spec=<span class="hljs-literal">None</span>,
)
Example:
layers.Input(shape=(blockSize*height,blockSize*width),dtype=np.float32)
</div></code></pre>
<h3 id="%E5%87%BD%E6%95%B0%E5%B1%82%E5%BB%BA%E9%80%A0%E4%B8%AD">函数层（建造中）</h3>
<pre class="hljs"><code><div>tf.keras.layers.Lambda
</div></code></pre>
<h2 id="%E9%87%8F%E5%8C%96">量化</h2>
<h3 id="%E4%BB%80%E4%B9%88%E6%98%AF%E9%87%8F%E5%8C%96">什么是量化</h3>
<p>简单来说，就是简化神经网络的结构（合并一些层），减小参数的数据量（比如说，把float32类型的权重变为int8型），通过一系列方法压缩模型体积来满足嵌入式设备的要求。</p>
<style>
        /*黑幕组件*/
        
        .black {
            display: inline;
            background-color: rgba(0, 0, 0, 1);
        }
        
        .black:hover {
            display: inline;
            background-color: rgba(0, 0, 0, 0);
        }
</style>
<script>
    /*LaTeX组件*/
        MathJax = {
            // 仅仅为标志符，表示在这个符号中间的内容为公式内容，官方文档多了['(',')']
            tex: {
                inlineMath: [
                    ['$', '$']
                ]
            }
        };
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
</body>
</html>
